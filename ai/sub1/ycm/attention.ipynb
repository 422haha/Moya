{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYTKC5SHYZqU"
      },
      "source": [
        "## 과제 목표\n",
        "1. Transformer의 multi-head self-attention의 이해 및 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywOhJ0R4YZqV"
      },
      "source": [
        "# 환경 세팅"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbS9YgsOYZqV"
      },
      "source": [
        "본 실습에서 사용할 라이브러리를 import 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ekvqlt1ljeUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HQL3hEx55x0J"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXRTjyeLYZqW"
      },
      "source": [
        "# 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1hQFJUy6dMl"
      },
      "source": [
        "본 실습에서 사용할 가상의 데이터를 만들겠습니다. 총 10개 시퀀스의 데이터가 이미 토큰화가 진행되어 주어졌다 가정하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "01KXLeH16cUF"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrkLne1BYZqW"
      },
      "source": [
        "## 1. Padding 함수 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL10MTzj6Vja"
      },
      "source": [
        "어텐션 연산에 들어갈 데이터의 전처리를 진행하겠습니다. 본 실습의 전처리 과정에서는 데이터의 길이를 균일하게 맞출 padding 함수를 구현합니다.\n",
        "\n",
        "배치 내 데이터 중 최대 길이에 맞도록 다른 데이터의 마지막에 임의의 패딩 값을 패딩해 길이를 맞추는 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "U3i0HSml6wIR"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "# Req 3-1: Padding 함수 구현하기                                             #\n",
        "############################################################################\n",
        "\n",
        "def padding(data, pad_value=0):\n",
        "    max_len = max([len(i) for i in data]) # 가장 긴 데이터의 길이\n",
        "    for i in range(len(data)):\n",
        "        if len(data[i]) < max_len:\n",
        "            data[i] += [pad_value] * (max_len - len(data[i]))\n",
        "\n",
        "    return data, max_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE0bvDHrYZqX"
      },
      "source": [
        "아래 코드로 패딩 결과를 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mO6N-pYJ7s9O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe3c5ed1-8575-464f-b88a-768a99e66abe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 20\n",
            "[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0]\n",
            "[60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10]\n",
            "[70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0]\n",
            "[20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "pad_value = 0 # 패딩(Padding)은 제로(zero) 패딩으로, 빈 공간을 0으로 채움.\n",
        "data, max_len = padding(data)\n",
        "print(f\"Maximum sequence length: {max_len}\")\n",
        "for d in data:\n",
        "    print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SapaNXN6N4Z"
      },
      "source": [
        "# Single-head Self-Attention 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRI6ROCIYZqX"
      },
      "source": [
        "Multi-head attention을 구현하기 앞서, 먼저 single-head attention을 구현해보겠습니다. 아래는 single-head self-attention을 위한 설정 값입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AG97Yr6FYZqX"
      },
      "outputs": [],
      "source": [
        "# 한 데이터의 최대 길이(vocab_size)는 100이라 가정.\n",
        "vocab_size = 100\n",
        "\n",
        "# hidden size of model\n",
        "d_k = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPhjzSZ1mykL"
      },
      "source": [
        "## 2. Embedding 및 Query, Key, Value Vector 구하기\n",
        "\n",
        "토큰화된 입력 데이터가 임베딩 과정을 거쳐 임베딩 공간의 `d_k` 차원으로 projection되어 임베딩 벡터가 되도록 임베딩 레이어를 정의합니다. PyTorch의 `nn.Embedding` 모듈을 이용할 수 있습니다.\n",
        "\n",
        "- 참고자료 (PyTorch Emedding): https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "\n",
        "이후, 임베딩 벡터를 각각 Query, Key, Value 벡터로 변환하는 Linear layer를 정의합니다. `d_k` 차원의 임베딩 벡터를 각각 `d_k` 차원을 가지는 3개의 벡터로 만들어야 합니다. PyTorch의 `nn.Linear` 모듈을 이용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltZATd6UnTz7",
        "outputId": "b8f9267f-1e2f-49d8-c7de-2936f1e14050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embedding: torch.Size([10, 20, 512])\n",
            "Shape of Q, K, V: (torch.Size([10, 20, 512]), torch.Size([10, 20, 512]), torch.Size([10, 20, 512]))\n"
          ]
        }
      ],
      "source": [
        "d_model = 512  # model의 hidden size\n",
        "num_heads = 8  # head의 개수\n",
        "\n",
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n",
        "# B: Batch, L: Max length of sequence\n",
        "batch = torch.LongTensor(data) # (B, L)\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)\n",
        "print(f\"Shape of embedding: {batch_emb.shape}\")\n",
        "\n",
        "# d_k 차원의 Q, K, V 벡터를 만들 레이어 정의\n",
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "# 임베딩 벡터를 Q, K, V 벡터로 변환\n",
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIm8qW0Spp5-"
      },
      "source": [
        "## 3. Scaled Dot-Product Self-Attention 구현하기\n",
        "\n",
        "![scaled-dot-product-atten](https://velog.velcdn.com/images/glad415/post/1645abbb-e260-4b82-ab1d-c8aa134ea8b7/image.png)\n",
        "\n",
        "각 head에서 진행하는 Q, K, V의 Self-attention을 구현하겠습니다.\n",
        "\n",
        "- 가장 먼저, 쿼리와 키(의 전치) 벡터를 곱한 뒤, 벡터 차원의 제곱근으로 나눕니다. 해당 과정은 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가를 연산합니다.\n",
        "- 그 값에 softmax 함수를 취합니다.\n",
        "- 그 값에 밸류 벡터를 곱하여 최종 attention matrix를 얻습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwhSwgDOp3Hf",
        "outputId": "c72959e9-4812-4a88-8edf-3c11086f4b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of attn_scores: torch.Size([10, 20, 20])\n",
            "Shape of attn_dists: torch.Size([10, 20, 20])\n",
            "Shape of attn_values: torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "############################################################################\n",
        "# Req 3-3: Scaled Dot-Product Self-Attention 구현하기                        #\n",
        "############################################################################\n",
        "\n",
        "################################################################################\n",
        "# TODO: 수식에 맞추어 scaled dot-product self-attention을 구현함                   #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n",
        "# 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n",
        "# 3) Value 벡터를 곱해 최종 attention value 계산\n",
        "\n",
        "# Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n",
        "# Output shape: (B, L, L)\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, T_L, S_L)\n",
        "print(f\"Shape of attn_scores: {attn_scores.shape}\")\n",
        "\n",
        "# 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n",
        "# Output shape: (B, L, L)\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, T_L, S_L)\n",
        "print(f\"Shape of attn_dists: {attn_dists.shape}\")\n",
        "\n",
        "# Value 벡터를 곱해 최종 attention value 계산\n",
        "# Output shape: (B, L, d_k)\n",
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, T_L, d_k)\n",
        "print(f\"Shape of attn_values: {attn_values.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT3oE93Pqxjl"
      },
      "source": [
        "# Multi-Head Self-Attention 구현하기\n",
        "\n",
        "![multi-head-attn](https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2F2e70e601-e268-4b55-a8c9-11b3ff145d92%2Fimage.png)\n",
        "\n",
        "Single-head 코드를 바탕으로 multi-head self-attention을 구현하겠습니다. 전체적으론 아래 내용이 변경 혹은 추가되어야 합니다.\n",
        "\n",
        "- Q, K, V를 (임베딩 벡터 차원) x (head 개수) 차원으로 projection한 뒤 각 head 개수로 쪼개 사용합니다.\n",
        "    - 임베딩 과정에서 입력 데이터를 임베딩 공간의 `d_model` 차원으로 projection합니다.\n",
        "    - 해당 `d_model` 값은 어텐션 헤드의 개수(`n_heads`)로 나누어 떨어져야 합니다.\n",
        "- 각 헤드의 연산을 한 뒤에는 각 헤드별로 가중치를 곱해 최종 attention value를 구합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVlNQZXQYZqY"
      },
      "source": [
        "아래는 multi-head self-attention 구현에 사용할 설정 값입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "vyLby_JsYZqY"
      },
      "outputs": [],
      "source": [
        "# 한 데이터의 최대 길이(vocab_size)는 100이라 가정.\n",
        "vocab_size = 100\n",
        "\n",
        "# hidden size of model\n",
        "d_k = 64\n",
        "\n",
        "# hidden size of model\n",
        "d_model = 512\n",
        "\n",
        "# number of attention heads\n",
        "num_heads = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjMDtdJEsYuq"
      },
      "source": [
        "## 4. Embedding 및 Query, Key, Value Vector 구하기\n",
        "\n",
        "- Embedding 시 주의사항\n",
        "  - Single-Head와 마찬가지로 데이터를 임베딩 벡터로 전환해야 합니다. 단, 해당 임베딩 벡터를 나중엔 헤드의 수대로 쪼개야 하기 때문에, 이 부분을 유의합니다.\n",
        "- Q, K, V 벡터 변환 시 주의사항\n",
        "  - 이론적으로는 multi-head attention을 수행할 때, input을 각각 다른 head 개수만큼의 `w_q`, `w_k`, `w_v`로 linear transformation하여 각각 여러 번의 attention을 수행한 후 concat하고 linear transformation을 수행합니다.\n",
        "  - 하지만 구현에서는 $W^Q$\\, $W^K$\\, $W^V$ 한 개씩만 사용합니다. 여러 헤드의 각 Q, K, V 벡터들을 한 번에 가진 긴 각 Q, K, V 벡터를 만드는 셈입니다.\n",
        "  - 따라서 각 linear layers `w_q`, `w_k`, `w_v` 는 `d_model` 차원의 입력 임베딩 벡터를 `d_model` 차원의 Q, K, V로 만들어야 합니다.\n",
        "  - 그리고 이렇게 긴 (여러 헤드의 값을 한 번에 가진) Q, K, V 벡터를 각 헤드만큼씩 쪼개야 합니다. 앞선 single-head attention 때와 마찬가지로, `d_k`는 각 헤드의 벡터의 차원으로, `d_model`이 헤드 개수 만큼씩 쪼개져 들어갑니다. 즉, 각 Q, K, V 벡터를 `d_model` → (num_heads, d_k)` 형태로 변경하는 작업을 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3YRujo-sTeo",
        "outputId": "6bc0c45d-397d-4a39-aa52-55547c4c0810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embedding: torch.Size([10, 20, 8])\n",
            "Shape of Q, K, V: (torch.Size([10, 20, 8]), torch.Size([10, 20, 8]), torch.Size([10, 20, 8]))\n",
            "Shape of Q, K, V: (torch.Size([10, 2, 20, 4]), torch.Size([10, 2, 20, 4]), torch.Size([10, 2, 20, 4]))\n"
          ]
        }
      ],
      "source": [
        "############################################################################\n",
        "# Req 3-4: Embedding 및 Query, Key, Value Vector 구하기                      #\n",
        "############################################################################\n",
        "\n",
        "################################################################################\n",
        "# TODO: Embedding 레이어를 정의하고, 입력 데이터를 임베딩화함                         #\n",
        "# 또한 Q, K, V를 위한 레이어를 정의하고, 임베딩 벡터를 각각 Q, K, V 벡터로 변환함       #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# 1) vocab_size 개수의 데이터 각각을 d_model 차원으로 표현할 임베딩을 할 레이어 정의\n",
        "# 2) 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n",
        "# 3) d_model 차원의 Q, K, V 벡터를 만들 레이어 정의\n",
        "# 4) 임베딩 벡터를 Q, K, V 벡터로 변환\n",
        "# 5) d_k 차원 계산\n",
        "# 6) Q, K, V 벡터를 각 헤드만큼 쪼갬 (각 헤드의 각 벡터 차원은 d_k)\n",
        "# 7) Self-attention 연산을 위하여 각 헤드가 (L, d_k) 행렬을 갖도록 축을 transpose\n",
        "\n",
        "# vocab_size 개수의 데이터 각각을 d_model 차원으로 표현할 임베딩을 할 레이어 정의\n",
        "d_model = 8  # model의 hidden size\n",
        "num_heads = 2  # head의 개수\n",
        "inf = 1e12\n",
        "\n",
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n",
        "# B: Batch, L: Max length of sequence\n",
        "batch = torch.LongTensor(data) # (B, L)\n",
        "batch_size = batch.shape[0]\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)\n",
        "print(f\"Shape of embedding: {batch_emb.shape}\")\n",
        "\n",
        "# d_k 차원의 Q, K, V 벡터를 만들 레이어 정의\n",
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n",
        "\n",
        "# d_k 차원 계산\n",
        "d_k = d_model // num_heads #\"\"\"Write your code\"\"\"\n",
        "\n",
        "# Q, K, V 벡터를 각 헤드만큼 쪼갬 (각 헤드의 각 벡터 차원은 d_k)\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "# Self-attention 연산을 위하여 각 헤드가 (L, d_k) 행렬을 갖도록 축을 transpose\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZACYsG4is6Pz"
      },
      "source": [
        "## 5. Scaled Dot-Product Self-Attention 구현하기\n",
        "\n",
        "해당 부분은 각 head에서 진행되는 attention 연산으로, 코드는 single-head와 동일합니다. 그러나 해당 연산이 이루어지는 matrix의 형태가 다릅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNFEEnM9s5xt",
        "outputId": "14f90ba8-de98-43ab-b504-72991e0ebac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of attn_scores: torch.Size([10, 2, 20, 20])\n",
            "Shape of attn_dists: torch.Size([10, 2, 20, 20])\n",
            "Shape of attn_values: torch.Size([10, 2, 20, 4])\n"
          ]
        }
      ],
      "source": [
        "############################################################################\n",
        "# Req 3-5: Scaled Dot-Product Self-Attention 구현하기                        #\n",
        "############################################################################\n",
        "\n",
        "################################################################################\n",
        "# TODO: 수식에 맞추어 scaled dot-product self-attention을 구현함                   #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n",
        "# 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n",
        "# 3) Value 벡터를 곱해 최종 attention value 계산\n",
        "\n",
        "# Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n",
        "# Output shape - (B, num_heads, L, L)\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "print(f\"Shape of attn_scores: {attn_scores.shape}\")\n",
        "\n",
        "# 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n",
        "# Output shape: (B, num_heads, L, L)\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "print(f\"Shape of attn_dists: {attn_dists.shape}\")\n",
        "\n",
        "# Value 벡터를 곱해 최종 attention value 계산\n",
        "# Output shape: (B, num_heads, L, d_k)\n",
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "print(f\"Shape of attn_values: {attn_values.shape}\")\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciMG2MXsqWhu"
      },
      "source": [
        "## 6. Attention heads의 결과물 병합하기\n",
        "\n",
        "각 attention head의 결과물을 concatenate해 병합하고, head 별로 정해진 가중치로 linear projection하여 최종 출력을 결정합니다.\n",
        "이 linear projection은 서로 다른 의미로 focusing된 각 head의 self-attention 정보를 합치는 역할을 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wAhW2_Qu9Qn",
        "outputId": "64ccf48b-b14d-4dcc-ce41-56f00de3d1a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of attention values from all heads: torch.Size([10, 20, 8])\n",
            "Shape of multi-head self-attention result: torch.Size([10, 20, 8])\n"
          ]
        }
      ],
      "source": [
        "############################################################################\n",
        "# Req 3-6: Attention heads의 결과물 병합하기                                  #\n",
        "############################################################################\n",
        "\n",
        "################################################################################\n",
        "# TODO: 각 attention head의 결과물을 병합하고 head 별 가중치로 projection하여        #\n",
        "# 최종 attention 결과를 구함                                                     #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# 1) Scaled dot-product 의 결과(attn_values)를 (B, L, num_heads, d_k) 형태로 축 변경\n",
        "# 2) 각 head의 점수를 concatenate함\n",
        "# 3) 각 head마다의 가중치를 부여할 linear layer 정의\n",
        "# 4) 위 linear projection layer를 통과하여 최종 출력을 계산\n",
        "\n",
        "\n",
        "# Scaled dot-product 의 결과(attn_values)를 (B, L, num_heads, d_k) 형태로 축 변경\n",
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
        "# 각 head의 점수를 concatenate함\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
        "print(f\"Shape of attention values from all heads: {attn_values.shape}\")\n",
        "\n",
        "# 각 head마다의 가중치를 부여할 linear layer 정의\n",
        "w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "# 위 linear projection layer를 통과하여 최종 출력을 계산\n",
        "outputs =  w_0(attn_values) # (B, L, d_model) -> (B, L, d_model)\n",
        "print(f\"Shape of multi-head self-attention result: {outputs.shape}\")\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVg9X9A9vilS"
      },
      "source": [
        "# 7. Multi-Head Self-Attention의 모듈 클래스 구현\n",
        "\n",
        "앞선 <Req. 3-4> ~ <Req. 3-6>의 multi-head self-attention 구현 과정을 하나의 모듈 클래스로 만들어 처리를 손쉽게 구현하겠습니다.\n",
        "\n",
        "해당 클래스의 forward 연산에서는 임베딩 벡터를 입력으로 받아 multi-head self-attention 결과를 출력하도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "9ypP4_Sjv1aX"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "# Req 3-7: Multi-Head Self-Attention의 모듈 클래스 구현                       #\n",
        "############################################################################\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, d_model, d_k, num_heads):\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        ################################################################################\n",
        "        # TODO: 조건에 맞게 모델의 레이어를 정의함                                          #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1) 임베딩 벡터를 Q, K, V 벡터로 변환할 레이어 정의\n",
        "        # 2) 각 헤드의 결과에 가중치를 부여할 linear layer 정의\n",
        "\n",
        "        # Q, K, V learnable matrices\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linear projection for concatenated outputs\n",
        "        self.w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "    def self_attention(self, q, k, v):\n",
        "      attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "      attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "      attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "      return attn_values\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "      batch_size = q.shape[0]\n",
        "\n",
        "      q = self.w_q(q)  # (B, L, d_model)\n",
        "      k = self.w_k(k)  # (B, L, d_model)\n",
        "      v = self.w_v(v)  # (B, L, d_model)\n",
        "\n",
        "      q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "      k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "      v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "      q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "      k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "      v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "      attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
        "      attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
        "\n",
        "      return self.w_0(attn_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoJIvnRfYZqZ"
      },
      "source": [
        "아래 코드로 결과를 테스트할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUj1DXQ7wFrz",
        "outputId": "0ca14454-9d2e-402c-ae57-4ed43a44ab4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 2.1264e-01,  3.5492e-01,  1.8304e-01,  ...,  8.7780e-02,\n",
            "          -2.5383e-01,  3.3153e-01],\n",
            "         [ 2.1991e-01,  3.5414e-01,  2.1071e-01,  ...,  7.6017e-02,\n",
            "          -2.6072e-01,  3.1015e-01],\n",
            "         [ 2.6274e-01,  3.8942e-01,  1.2387e-01,  ...,  8.6687e-02,\n",
            "          -2.3916e-01,  2.7369e-01],\n",
            "         ...,\n",
            "         [ 2.1946e-01,  4.0725e-01,  1.2190e-01,  ...,  8.9938e-02,\n",
            "          -2.5376e-01,  2.9072e-01],\n",
            "         [ 2.1946e-01,  4.0725e-01,  1.2190e-01,  ...,  8.9938e-02,\n",
            "          -2.5376e-01,  2.9072e-01],\n",
            "         [ 2.1946e-01,  4.0725e-01,  1.2190e-01,  ...,  8.9938e-02,\n",
            "          -2.5376e-01,  2.9072e-01]],\n",
            "\n",
            "        [[ 4.0351e-01,  1.1209e+00, -3.7386e-01,  ...,  2.4559e-01,\n",
            "          -4.6360e-01,  2.4086e-01],\n",
            "         [ 3.7780e-01,  1.1147e+00, -3.5091e-01,  ...,  2.4403e-01,\n",
            "          -4.5455e-01,  2.4329e-01],\n",
            "         [ 3.8427e-01,  1.0859e+00, -3.3563e-01,  ...,  2.3370e-01,\n",
            "          -4.5099e-01,  2.5644e-01],\n",
            "         ...,\n",
            "         [ 3.9570e-01,  1.0967e+00, -3.5754e-01,  ...,  2.3435e-01,\n",
            "          -4.4890e-01,  2.4099e-01],\n",
            "         [ 3.9570e-01,  1.0967e+00, -3.5754e-01,  ...,  2.3435e-01,\n",
            "          -4.4890e-01,  2.4099e-01],\n",
            "         [ 3.9570e-01,  1.0967e+00, -3.5754e-01,  ...,  2.3435e-01,\n",
            "          -4.4890e-01,  2.4099e-01]],\n",
            "\n",
            "        [[ 2.4431e-01,  8.8420e-01, -1.0203e-01,  ...,  1.7081e-01,\n",
            "          -3.8531e-01,  1.8526e-01],\n",
            "         [ 2.8337e-01,  9.7989e-01, -1.7712e-01,  ...,  2.2499e-01,\n",
            "          -4.6410e-01,  2.5366e-01],\n",
            "         [ 2.0414e-01,  7.8236e-01, -4.4981e-02,  ...,  1.3914e-01,\n",
            "          -3.0133e-01,  1.1799e-01],\n",
            "         ...,\n",
            "         [ 2.7190e-01,  9.1967e-01, -1.3753e-01,  ...,  1.8734e-01,\n",
            "          -4.1552e-01,  2.0983e-01],\n",
            "         [ 2.7190e-01,  9.1967e-01, -1.3753e-01,  ...,  1.8734e-01,\n",
            "          -4.1552e-01,  2.0983e-01],\n",
            "         [ 2.7190e-01,  9.1967e-01, -1.3753e-01,  ...,  1.8734e-01,\n",
            "          -4.1552e-01,  2.0983e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-5.0784e-02,  4.4630e-01,  4.2424e-01,  ...,  9.1375e-03,\n",
            "          -1.7383e-01,  2.5143e-01],\n",
            "         [ 4.0051e-02,  4.5280e-01,  5.1024e-01,  ...,  5.5929e-04,\n",
            "          -1.2732e-01,  1.7003e-01],\n",
            "         [-5.6630e-02,  5.0533e-01,  3.6850e-01,  ...,  1.5305e-02,\n",
            "          -1.8325e-01,  2.1648e-01],\n",
            "         ...,\n",
            "         [ 2.2224e-02,  5.5480e-01,  4.7322e-01,  ...,  4.8509e-02,\n",
            "          -2.1287e-01,  2.4676e-01],\n",
            "         [-1.2277e-02,  5.6016e-01,  3.2434e-01,  ...,  3.4544e-02,\n",
            "          -1.3413e-01,  1.3240e-01],\n",
            "         [-7.8539e-03,  4.3881e-01,  4.5535e-01,  ..., -1.3071e-02,\n",
            "          -1.1422e-01,  1.5370e-01]],\n",
            "\n",
            "        [[ 2.3140e-01,  6.3035e-01,  5.3661e-02,  ...,  1.6091e-01,\n",
            "          -3.5763e-01,  3.5097e-01],\n",
            "         [ 2.2765e-01,  4.8378e-01,  2.0278e-01,  ...,  1.0367e-01,\n",
            "          -3.0524e-01,  3.7424e-01],\n",
            "         [ 2.4373e-01,  7.3490e-01, -2.9283e-02,  ...,  1.8569e-01,\n",
            "          -3.8109e-01,  3.3025e-01],\n",
            "         ...,\n",
            "         [ 1.8235e-01,  6.3842e-01,  9.8940e-02,  ...,  1.4317e-01,\n",
            "          -3.4980e-01,  3.5512e-01],\n",
            "         [ 1.8235e-01,  6.3842e-01,  9.8940e-02,  ...,  1.4317e-01,\n",
            "          -3.4980e-01,  3.5512e-01],\n",
            "         [ 1.8235e-01,  6.3842e-01,  9.8940e-02,  ...,  1.4317e-01,\n",
            "          -3.4980e-01,  3.5512e-01]],\n",
            "\n",
            "        [[ 2.4078e-01,  7.8493e-01, -8.9373e-03,  ...,  2.1624e-01,\n",
            "          -3.9838e-01,  3.1400e-01],\n",
            "         [ 2.6422e-01,  6.5564e-01,  1.1466e-01,  ...,  1.4874e-01,\n",
            "          -3.4786e-01,  2.7902e-01],\n",
            "         [ 3.0235e-01,  7.2850e-01,  9.3788e-03,  ...,  1.8940e-01,\n",
            "          -3.5950e-01,  2.5963e-01],\n",
            "         ...,\n",
            "         [ 2.0915e-01,  7.6454e-01,  3.5593e-02,  ...,  1.8102e-01,\n",
            "          -3.6601e-01,  2.9115e-01],\n",
            "         [ 2.0915e-01,  7.6454e-01,  3.5593e-02,  ...,  1.8102e-01,\n",
            "          -3.6601e-01,  2.9115e-01],\n",
            "         [ 2.0915e-01,  7.6454e-01,  3.5593e-02,  ...,  1.8102e-01,\n",
            "          -3.6601e-01,  2.9115e-01]]], grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 8])\n"
          ]
        }
      ],
      "source": [
        "multi_attn = MultiheadAttention(d_model, d_k, num_heads)\n",
        "outputs = multi_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}