{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "temxFdL1TLPp"
      },
      "source": [
        "## 과제 목표\n",
        "1. Transformer의 multi-head self-attention의 이해 및 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uri2RazhTLPq"
      },
      "source": [
        "# 환경 세팅"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtfcm9sITLPr"
      },
      "source": [
        "본 실습에서 사용할 라이브러리를 import 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HQL3hEx55x0J"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8OR30D4TLPr"
      },
      "source": [
        "# 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1hQFJUy6dMl"
      },
      "source": [
        "본 실습에서 사용할 가상의 데이터를 만들겠습니다. 총 10개 시퀀스의 데이터가 이미 토큰화가 진행되어 주어졌다 가정하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "01KXLeH16cUF"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHTGWf9qTLPs"
      },
      "source": [
        "## 1. Padding 함수 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL10MTzj6Vja"
      },
      "source": [
        "어텐션 연산에 들어갈 데이터의 전처리를 진행하겠습니다. 본 실습의 전처리 과정에서는 데이터의 길이를 균일하게 맞출 padding 함수를 구현합니다.\n",
        "\n",
        "배치 내 데이터 중 최대 길이에 맞도록 다른 데이터의 마지막에 임의의 패딩 값을 패딩해 길이를 맞추는 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U3i0HSml6wIR"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "# Req 3-1: Padding 함수 구현하기                                             #\n",
        "############################################################################\n",
        "\n",
        "def padding(data, pad_value=0):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        data (list[list[int]]): data sequence\n",
        "        pad_value (int): value to pad\n",
        "    Returns:\n",
        "        data (list[list[int]]): padded data sequence\n",
        "        max_len (int): maximum length of intput data\n",
        "    \"\"\"\n",
        "    ################################################################################\n",
        "    # TODO: 배치 내 데이터 중 최대 길이에 맞추어 다른 데이터의 뒷부분에 패딩 값을 붙여        #\n",
        "    # 길이를 동일하게 맞추어 반환함                                                     #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # 1) 가장 긴 데이터의 길이 추출\n",
        "    max_len = max(len(seq) for seq in data)\n",
        "    # 2) 모든 데이터를 돌면서 1에서 구한 길이에 맞게 뒷부분을 pad_value로 채우기\n",
        "    padded_data = [seq + [pad_value] * (max_len - len(seq)) for seq in data]\n",
        "\n",
        "    \"\"\"Write your code\"\"\"\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        "\n",
        "    return padded_data, max_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAUIzMWvTLPs"
      },
      "source": [
        "아래 코드로 패딩 결과를 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mO6N-pYJ7s9O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2e4057-85ef-486f-f0eb-35acb056b4de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 20\n",
            "[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0]\n",
            "[60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10]\n",
            "[70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0]\n",
            "[20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "pad_value = 0 # 패딩(Padding)은 제로(zero) 패딩으로, 빈 공간을 0으로 채움.\n",
        "data, max_len = padding(data)\n",
        "print(f\"Maximum sequence length: {max_len}\")\n",
        "for d in data:\n",
        "    print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SapaNXN6N4Z"
      },
      "source": [
        "# Single-head Self-Attention 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIhGoOYGTLPs"
      },
      "source": [
        "Multi-head attention을 구현하기 앞서, 먼저 single-head attention을 구현해보겠습니다. 아래는 single-head self-attention을 위한 설정 값입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qubNxD1uTLPs"
      },
      "outputs": [],
      "source": [
        "# 한 데이터의 최대 길이(vocab_size)는 100이라 가정.\n",
        "vocab_size = 100\n",
        "\n",
        "# hidden size of model\n",
        "d_k = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPhjzSZ1mykL"
      },
      "source": [
        "## 2. Embedding 및 Query, Key, Value Vector 구하기\n",
        "\n",
        "토큰화된 입력 데이터가 임베딩 과정을 거쳐 임베딩 공간의 `d_k` 차원으로 projection되어 임베딩 벡터가 되도록 임베딩 레이어를 정의합니다. PyTorch의 `nn.Embedding` 모듈을 이용할 수 있습니다.\n",
        "\n",
        "- 참고자료 (PyTorch Emedding): https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "\n",
        "이후, 임베딩 벡터를 각각 Query, Key, Value 벡터로 변환하는 Linear layer를 정의합니다. `d_k` 차원의 임베딩 벡터를 각각 `d_k` 차원을 가지는 3개의 벡터로 만들어야 합니다. PyTorch의 `nn.Linear` 모듈을 이용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltZATd6UnTz7",
        "outputId": "0509c169-963f-4a93-9b85-dcf8069c5bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embedding: torch.Size([10, 20, 64])\n",
            "Shape of Q, K, V: (torch.Size([10, 20, 64]), torch.Size([10, 20, 64]), torch.Size([10, 20, 64]))\n"
          ]
        }
      ],
      "source": [
        "############################################################################\n",
        "# Req 3-2: Embedding 및 Query, Key, Value Vector 구하기                     #\n",
        "############################################################################\n",
        "\n",
        "################################################################################\n",
        "# TODO: Embedding 레이어를 정의하고, 입력 데이터를 임베딩화함                         #\n",
        "# 또한 Q, K, V를 위한 레이어를 정의하고, 임베딩 벡터를 각각 Q, K, V 벡터로 변환함       #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# 1) vocab_size 개수의 데이터 각각을 d_k 차원으로 표현할 임베딩을 할 레이어 정의\n",
        "# 2) 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n",
        "# 3) d_k 차원의 Q, K, V 벡터를 만들 레이어 정의\n",
        "# 4) 임베딩 벡터를 Q, K, V 벡터로 변환\n",
        "\n",
        "# vocab_size 개수의 데이터 각각을 d_k 차원으로 표현할 임베딩을 할 레이어 정의\n",
        "\"\"\"Write your code\"\"\"\n",
        "embedding_layer = nn.Embedding(vocab_size, d_k)\n",
        "\n",
        "# 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n",
        "# B: Batch, L: Max length of sequence\n",
        "batch = torch.LongTensor(data) # (B, L)\n",
        "batch_emb = embedding_layer(batch) # (B, L, d_k)\n",
        "print(f\"Shape of embedding: {batch_emb.shape}\")\n",
        "\n",
        "# d_k 차원의 Q, K, V 벡터를 만들 레이어 정의\n",
        "w_q = nn.Linear(d_k, d_k)\n",
        "w_k = nn.Linear(d_k, d_k)\n",
        "w_v = nn.Linear(d_k, d_k)\n",
        "\n",
        "# 임베딩 벡터를 Q, K, V 벡터로 변환\n",
        "q = w_q(batch_emb)  # (B, L, d_k)\n",
        "k = w_k(batch_emb)  # (B, L, d_k)\n",
        "v = w_v(batch_emb)  # (B, L, d_k)\n",
        "print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIm8qW0Spp5-"
      },
      "source": [
        "## 3. Scaled Dot-Product Self-Attention 구현하기\n",
        "\n",
        "![scaled-dot-product-atten](https://velog.velcdn.com/images/glad415/post/1645abbb-e260-4b82-ab1d-c8aa134ea8b7/image.png)\n",
        "\n",
        "각 head에서 진행하는 Q, K, V의 Self-attention을 구현하겠습니다.\n",
        "\n",
        "- 가장 먼저, 쿼리와 키(의 전치) 벡터를 곱한 뒤, 벡터 차원의 제곱근으로 나눕니다. 해당 과정은 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가를 연산합니다.\n",
        "- 그 값에 softmax 함수를 취합니다.\n",
        "- 그 값에 밸류 벡터를 곱하여 최종 attention matrix를 얻습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwhSwgDOp3Hf",
        "outputId": "738dbd14-bbd4-4ef8-f24b-a5ae0ac17d6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of attn_scores: torch.Size([10, 20, 20])\n",
            "Shape of attn_dists: torch.Size([10, 20, 20])\n",
            "Shape of attn_values: torch.Size([10, 20, 64])\n"
          ]
        }
      ],
      "source": [
        "############################################################################\n",
        "# Req 3-3: Scaled Dot-Product Self-Attention 구현하기                        #\n",
        "############################################################################\n",
        "\n",
        "################################################################################\n",
        "# TODO: 수식에 맞추어 scaled dot-product self-attention을 구현함                   #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n",
        "# 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n",
        "# 3) Value 벡터를 곱해 최종 attention value 계산\n",
        "\n",
        "# Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n",
        "# Output shape: (B, L, L)\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "print(f\"Shape of attn_scores: {attn_scores.shape}\")\n",
        "\n",
        "# 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n",
        "# Output shape: (B, L, L)\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)\n",
        "print(f\"Shape of attn_dists: {attn_dists.shape}\")\n",
        "\n",
        "# Value 벡터를 곱해 최종 attention value 계산\n",
        "# Output shape: (B, L, d_k)\n",
        "attn_values = torch.matmul(attn_dists, v)\n",
        "print(f\"Shape of attn_values: {attn_values.shape}\")\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT3oE93Pqxjl"
      },
      "source": [
        "# Multi-Head Self-Attention 구현하기\n",
        "\n",
        "![multi-head-attn](https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2F2e70e601-e268-4b55-a8c9-11b3ff145d92%2Fimage.png)\n",
        "\n",
        "Single-head 코드를 바탕으로 multi-head self-attention을 구현하겠습니다. 전체적으론 아래 내용이 변경 혹은 추가되어야 합니다.\n",
        "\n",
        "- Q, K, V를 (임베딩 벡터 차원) x (head 개수) 차원으로 projection한 뒤 각 head 개수로 쪼개 사용합니다.\n",
        "    - 임베딩 과정에서 입력 데이터를 임베딩 공간의 `d_model` 차원으로 projection합니다.\n",
        "    - 해당 `d_model` 값은 어텐션 헤드의 개수(`n_heads`)로 나누어 떨어져야 합니다.\n",
        "- 각 헤드의 연산을 한 뒤에는 각 헤드별로 가중치를 곱해 최종 attention value를 구합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQoY8FaYTLPt"
      },
      "source": [
        "아래는 multi-head self-attention 구현에 사용할 설정 값입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nVq0J8snTLPt"
      },
      "outputs": [],
      "source": [
        "# 한 데이터의 최대 길이(vocab_size)는 100이라 가정.\n",
        "vocab_size = 100\n",
        "\n",
        "# hidden size of model\n",
        "d_k = 64\n",
        "\n",
        "# hidden size of model\n",
        "d_model = 512\n",
        "\n",
        "# number of attention heads\n",
        "num_heads = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjMDtdJEsYuq"
      },
      "source": [
        "## 4. Embedding 및 Query, Key, Value Vector 구하기\n",
        "\n",
        "- Embedding 시 주의사항\n",
        "  - Single-Head와 마찬가지로 데이터를 임베딩 벡터로 전환해야 합니다. 단, 해당 임베딩 벡터를 나중엔 헤드의 수대로 쪼개야 하기 때문에, 이 부분을 유의합니다.\n",
        "- Q, K, V 벡터 변환 시 주의사항\n",
        "  - 이론적으로는 multi-head attention을 수행할 때, input을 각각 다른 head 개수만큼의 `w_q`, `w_k`, `w_v`로 linear transformation하여 각각 여러 번의 attention을 수행한 후 concat하고 linear transformation을 수행합니다.\n",
        "  - 하지만 구현에서는 $W^Q$\\, $W^K$\\, $W^V$ 한 개씩만 사용합니다. 여러 헤드의 각 Q, K, V 벡터들을 한 번에 가진 긴 각 Q, K, V 벡터를 만드는 셈입니다.\n",
        "  - 따라서 각 linear layers `w_q`, `w_k`, `w_v` 는 `d_model` 차원의 입력 임베딩 벡터를 `d_model` 차원의 Q, K, V로 만들어야 합니다.\n",
        "  - 그리고 이렇게 긴 (여러 헤드의 값을 한 번에 가진) Q, K, V 벡터를 각 헤드만큼씩 쪼개야 합니다. 앞선 single-head attention 때와 마찬가지로, `d_k`는 각 헤드의 벡터의 차원으로, `d_model`이 헤드 개수 만큼씩 쪼개져 들어갑니다. 즉, 각 Q, K, V 벡터를 `d_model` → (num_heads, d_k)` 형태로 변경하는 작업을 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3YRujo-sTeo",
        "outputId": "a52e9701-4aea-42d6-82d6-8aa903a9882d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embedding: torch.Size([10, 20, 512])\n",
            "Shape of Q, K, V: (torch.Size([10, 20, 512]), torch.Size([10, 20, 512]), torch.Size([10, 20, 512]))\n",
            "Shape of Q, K, V: (torch.Size([10, 8, 20, 64]), torch.Size([10, 8, 20, 64]), torch.Size([10, 8, 20, 64]))\n"
          ]
        }
      ],
      "source": [
        "############################################################################\n",
        "# Req 3-4: Embedding 및 Query, Key, Value Vector 구하기                      #\n",
        "############################################################################\n",
        "\n",
        "################################################################################\n",
        "# TODO: Embedding 레이어를 정의하고, 입력 데이터를 임베딩화함                         #\n",
        "# 또한 Q, K, V를 위한 레이어를 정의하고, 임베딩 벡터를 각각 Q, K, V 벡터로 변환함       #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# 1) vocab_size 개수의 데이터 각각을 d_model 차원으로 표현할 임베딩을 할 레이어 정의\n",
        "# 2) 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n",
        "# 3) d_model 차원의 Q, K, V 벡터를 만들 레이어 정의\n",
        "# 4) 임베딩 벡터를 Q, K, V 벡터로 변환\n",
        "# 5) d_k 차원 계산\n",
        "# 6) Q, K, V 벡터를 각 헤드만큼 쪼갬 (각 헤드의 각 벡터 차원은 d_k)\n",
        "# 7) Self-attention 연산을 위하여 각 헤드가 (L, d_k) 행렬을 갖도록 축을 transpose\n",
        "\n",
        "# vocab_size 개수의 데이터 각각을 d_model 차원으로 표현할 임베딩을 할 레이어 정의\n",
        "embedding_layer = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n",
        "# B: Batch, L: Max length of sequence\n",
        "batch = torch.LongTensor(data) # (B, L)\n",
        "batch_size = batch.shape[0]\n",
        "batch_emb = embedding_layer(batch) # (B, L, d_model)\n",
        "print(f\"Shape of embedding: {batch_emb.shape}\")\n",
        "\n",
        "# d_k 차원의 Q, K, V 벡터를 만들 레이어 정의\n",
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "# 임베딩 벡터를 Q, K, V 벡터로 변환\n",
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n",
        "\n",
        "# d_k 차원 계산\n",
        "d_k = d_model // num_heads #\"\"\"Write your code\"\"\"\n",
        "\n",
        "# Q, K, V 벡터를 각 헤드만큼 쪼갬 (각 헤드의 각 벡터 차원은 d_k)\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k) # (B, L, num_heads, d_k)\n",
        "\n",
        "# Self-attention 연산을 위하여 각 헤드가 (L, d_k) 행렬을 갖도록 축을 transpose\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZACYsG4is6Pz"
      },
      "source": [
        "## 5. Scaled Dot-Product Self-Attention 구현하기\n",
        "\n",
        "해당 부분은 각 head에서 진행되는 attention 연산으로, 코드는 single-head와 동일합니다. 그러나 해당 연산이 이루어지는 matrix의 형태가 다릅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNFEEnM9s5xt",
        "outputId": "868de291-44d8-4503-afea-b6e677e773df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of attn_scores: torch.Size([10, 8, 20, 20])\n",
            "Shape of attn_dists: torch.Size([10, 8, 20, 20])\n",
            "Shape of attn_values: torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ],
      "source": [
        "############################################################################\n",
        "# Req 3-5: Scaled Dot-Product Self-Attention 구현하기                        #\n",
        "############################################################################\n",
        "\n",
        "################################################################################\n",
        "# TODO: 수식에 맞추어 scaled dot-product self-attention을 구현함                   #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n",
        "# 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n",
        "# 3) Value 벡터를 곱해 최종 attention value 계산\n",
        "\n",
        "# Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n",
        "# Output shape - (B, num_heads, L, L)\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "print(f\"Shape of attn_scores: {attn_scores.shape}\")\n",
        "\n",
        "# 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n",
        "# Output shape: (B, num_heads, L, L)\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)\n",
        "print(f\"Shape of attn_dists: {attn_dists.shape}\")\n",
        "\n",
        "# Value 벡터를 곱해 최종 attention value 계산\n",
        "# Output shape: (B, num_heads, L, d_k)\n",
        "attn_values = torch.matmul(attn_dists, v)\n",
        "print(f\"Shape of attn_values: {attn_values.shape}\")\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciMG2MXsqWhu"
      },
      "source": [
        "## 6. Attention heads의 결과물 병합하기\n",
        "\n",
        "각 attention head의 결과물을 concatenate해 병합하고, head 별로 정해진 가중치로 linear projection하여 최종 출력을 결정합니다.\n",
        "이 linear projection은 서로 다른 의미로 focusing된 각 head의 self-attention 정보를 합치는 역할을 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wAhW2_Qu9Qn",
        "outputId": "bf3fa526-f707-42d9-b488-0435805076f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of attention values from all heads: torch.Size([10, 20, 512])\n",
            "Shape of multi-head self-attention result: torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "############################################################################\n",
        "# Req 3-6: Attention heads의 결과물 병합하기                                  #\n",
        "############################################################################\n",
        "\n",
        "################################################################################\n",
        "# TODO: 각 attention head의 결과물을 병합하고 head 별 가중치로 projection하여        #\n",
        "# 최종 attention 결과를 구함                                                     #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# 1) Scaled dot-product 의 결과(attn_values)를 (B, L, num_heads, d_k) 형태로 축 변경\n",
        "# 2) 각 head의 점수를 concatenate함\n",
        "# 3) 각 head마다의 가중치를 부여할 linear layer 정의\n",
        "# 4) 위 linear projection layer를 통과하여 최종 출력을 계산\n",
        "\n",
        "\n",
        "# Scaled dot-product 의 결과(attn_values)를 (B, L, num_heads, d_k) 형태로 축 변경\n",
        "attn_values = attn_values.transpose(1, 2).contiguous() # (B, L, num_heads, d_k)\n",
        "\n",
        "# 각 head의 점수를 concatenate함\n",
        "attn_values = attn_values.view(batch_size, -1, d_model) # (B, L, d_model)\n",
        "print(f\"Shape of attention values from all heads: {attn_values.shape}\")\n",
        "\n",
        "# 각 head마다의 가중치를 부여할 linear layer 정의\n",
        "w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "# 위 linear projection layer를 통과하여 최종 출력을 계산\n",
        "outputs = w_0(attn_values) # (B, L, d_model) -> (B, L, d_model)\n",
        "print(f\"Shape of multi-head self-attention result: {outputs.shape}\")\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVg9X9A9vilS"
      },
      "source": [
        "# 7. Multi-Head Self-Attention의 모듈 클래스 구현\n",
        "\n",
        "앞선 <Req. 3-4> ~ <Req. 3-6>의 multi-head self-attention 구현 과정을 하나의 모듈 클래스로 만들어 처리를 손쉽게 구현하겠습니다.\n",
        "\n",
        "해당 클래스의 forward 연산에서는 임베딩 벡터를 입력으로 받아 multi-head self-attention 결과를 출력하도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9ypP4_Sjv1aX"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "# Req 3-7: Multi-Head Self-Attention의 모듈 클래스 구현                       #\n",
        "############################################################################\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, d_model, d_k, num_heads):\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        ################################################################################\n",
        "        # TODO: 조건에 맞게 모델의 레이어를 정의함                                          #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1) 임베딩 벡터를 Q, K, V 벡터로 변환할 레이어 정의\n",
        "        # 2) 각 헤드의 결과에 가중치를 부여할 linear layer 정의\n",
        "\n",
        "        # Q, K, V learnable matrices\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linear projection for concatenated outputs\n",
        "        self.w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "    def self_attention(self, q, k, v):\n",
        "        \"\"\"scaled-dot product attention\n",
        "\n",
        "        Args:\n",
        "            q, k, v (torch.Tensor): Query, Key, Value vectors\n",
        "        Returns:\n",
        "            attn_values (torch.Tensor): Result of self-attention\n",
        "        \"\"\"\n",
        "        ################################################################################\n",
        "        # TODO: Scaled dot-product self-attention 연산을 정의함                          #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        # 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n",
        "        attn_dists = F.softmax(attn_scores, dim=-1)\n",
        "        # 3) Value 벡터를 곱해 최종 attention value 계산\n",
        "        attn_values = torch.matmul(attn_dists, v)\n",
        "\n",
        "        \"\"\"Write your code\"\"\"\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "        return attn_values\n",
        "\n",
        "    def forward(self, batch_emb):\n",
        "        batch_size = batch_emb.shape[0]\n",
        "\n",
        "        ################################################################################\n",
        "        # TODO: Multi-head self-attention 과정을 정의함                                  #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # 1) 입력 데이터를 Q, K, V 벡터로 변환\n",
        "        q = self.w_q(batch_emb)\n",
        "        k = self.w_k(batch_emb)\n",
        "        v = self.w_v(batch_emb)\n",
        "        # 2) 위 결과를 Head의 수로 분할함\n",
        "        q = q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = k.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = v.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        # 3) 각 head가 (L, d_k)의 matrix를 담당하도록 만듦\n",
        "        attn_values = self.self_attention(q, k, v)\n",
        "        # 4) Scaled dot-product self-attention 연산을 수행함\n",
        "        attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)  # (B, L, d_model)\n",
        "        # 5) 각 attention head의 결과물을 concatenate해 병합함\n",
        "        attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)  # (B, L, d_model)\n",
        "        # 6) head 별로 정해진 가중치로 linear projection하여 최종 출력을 결정함\n",
        "        outputs = self.w_0(attn_values)\n",
        "\n",
        "        \"\"\"Write your code\"\"\"\n",
        "\n",
        "        return outputs\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSwNEkLhTLPu"
      },
      "source": [
        "아래 코드로 결과를 테스트할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUj1DXQ7wFrz",
        "outputId": "d005034b-b813-4b2c-d8ae-184de6ff6c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.1149e-01, -1.4003e-01, -1.5977e-01,  ..., -3.6184e-02,\n",
            "           6.4532e-02,  2.8890e-02],\n",
            "         [ 5.6419e-03, -1.0260e-01, -1.7129e-01,  ...,  1.0077e-01,\n",
            "           3.7781e-01,  1.8182e-01],\n",
            "         [ 9.6639e-02,  2.1157e-01,  5.3635e-03,  ...,  2.2680e-02,\n",
            "           1.5588e-01, -4.7856e-02],\n",
            "         ...,\n",
            "         [-7.8523e-03,  1.4162e-02,  3.7651e-03,  ..., -7.4718e-03,\n",
            "          -1.5981e-01,  7.3009e-02],\n",
            "         [-2.3915e-04, -3.2374e-02, -8.0590e-02,  ..., -5.6668e-02,\n",
            "           1.6484e-01,  1.6859e-01],\n",
            "         [ 3.8643e-03, -2.2126e-02,  1.0567e-01,  ..., -5.9150e-02,\n",
            "          -6.3716e-02,  6.0834e-03]],\n",
            "\n",
            "        [[-3.6583e-02, -1.1749e-01, -1.0227e-01,  ...,  2.4818e-02,\n",
            "           1.3746e-01,  1.5469e-02],\n",
            "         [ 1.3860e-01, -2.2828e-01, -2.5801e-01,  ...,  4.4276e-01,\n",
            "           9.3061e-01,  5.5674e-02],\n",
            "         [ 1.5021e-01, -1.1371e-01, -1.2750e-01,  ..., -1.2589e-01,\n",
            "           8.3690e-02, -3.4138e-01],\n",
            "         ...,\n",
            "         [ 2.1684e-01, -5.1105e-02,  2.0477e-01,  ...,  6.8619e-02,\n",
            "          -4.7488e-01,  1.5600e-01],\n",
            "         [-1.1059e-01,  2.6130e-01,  9.9339e-02,  ...,  6.2891e-02,\n",
            "           1.7885e-01,  5.6678e-02],\n",
            "         [ 2.9283e-01, -1.2957e-01, -1.4669e-01,  ..., -4.7253e-01,\n",
            "          -3.6492e-01, -1.7393e-01]],\n",
            "\n",
            "        [[-8.1584e-02, -1.5118e-02,  5.5795e-04,  ..., -2.1318e-03,\n",
            "           3.1042e-02,  8.4046e-02],\n",
            "         [ 9.8592e-02, -1.3167e-01, -1.8747e-01,  ...,  3.0568e-01,\n",
            "           6.3905e-01,  5.0317e-02],\n",
            "         [ 6.7569e-02, -2.9126e-02, -3.3621e-02,  ...,  2.6381e-03,\n",
            "           1.0421e-01, -1.6625e-01],\n",
            "         ...,\n",
            "         [ 1.0434e-01,  1.7337e-02,  1.6990e-01,  ...,  1.0657e-01,\n",
            "          -2.9304e-01,  1.4982e-01],\n",
            "         [-4.8576e-02,  1.7211e-01,  7.6166e-02,  ...,  3.6612e-02,\n",
            "           1.1997e-01, -1.2492e-02],\n",
            "         [ 1.4676e-01, -1.6178e-01, -5.4147e-02,  ..., -1.6859e-01,\n",
            "          -1.6741e-01, -1.0701e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.1726e-02,  1.0176e-01, -4.8927e-02,  ...,  2.3098e-02,\n",
            "           5.2865e-02,  4.1714e-02],\n",
            "         [-9.3576e-02,  1.2827e-01,  4.1973e-02,  ...,  1.0996e-01,\n",
            "           1.0019e-01, -1.1883e-01],\n",
            "         [-4.7391e-02,  1.0510e-03,  9.6778e-02,  ..., -2.3453e-02,\n",
            "           1.1298e-01,  1.2437e-02],\n",
            "         ...,\n",
            "         [ 1.0888e-01,  1.6263e-01,  1.0369e-01,  ...,  1.4325e-01,\n",
            "           7.0347e-02,  1.4076e-01],\n",
            "         [-4.5684e-02,  7.5032e-02,  4.0357e-02,  ...,  1.0752e-01,\n",
            "           1.3332e-03,  1.6097e-01],\n",
            "         [ 3.4043e-02,  1.7457e-02,  1.0501e-01,  ...,  7.1812e-02,\n",
            "          -8.8038e-03,  1.1474e-01]],\n",
            "\n",
            "        [[ 9.8108e-02, -6.7390e-02, -5.1622e-02,  ...,  1.4471e-01,\n",
            "           2.0826e-02,  1.0140e-01],\n",
            "         [ 1.8880e-01,  2.5525e-02, -1.4822e-01,  ..., -5.3862e-02,\n",
            "           3.1627e-01,  4.2161e-02],\n",
            "         [ 5.5391e-02,  6.8128e-02, -3.7748e-02,  ...,  3.3278e-02,\n",
            "           1.2069e-01, -5.1243e-02],\n",
            "         ...,\n",
            "         [ 1.7343e-01,  4.3907e-02,  6.3192e-02,  ...,  6.7719e-02,\n",
            "          -8.5584e-02, -1.9182e-02],\n",
            "         [ 3.1216e-02,  1.2832e-01,  1.3805e-02,  ..., -1.3408e-01,\n",
            "           2.2999e-02, -1.6639e-02],\n",
            "         [ 2.2329e-02,  4.5135e-02, -6.4957e-02,  ...,  2.4873e-02,\n",
            "          -6.8108e-02,  1.7644e-02]],\n",
            "\n",
            "        [[ 3.2900e-02, -5.1779e-02, -1.0087e-01,  ...,  2.8157e-02,\n",
            "           1.1085e-01,  2.5215e-02],\n",
            "         [ 1.3844e-01, -5.4808e-02, -1.8872e-01,  ...,  2.2348e-01,\n",
            "           4.4884e-01, -2.1729e-02],\n",
            "         [ 2.0189e-02,  1.4749e-01,  1.1644e-01,  ..., -2.0582e-02,\n",
            "           6.4615e-02, -3.3386e-02],\n",
            "         ...,\n",
            "         [ 2.1302e-01,  1.4487e-02,  1.0059e-01,  ..., -2.8333e-02,\n",
            "          -1.6197e-01, -7.5095e-03],\n",
            "         [-1.2856e-02,  1.2564e-01,  3.5336e-02,  ..., -1.0229e-01,\n",
            "           7.0867e-02,  7.8818e-02],\n",
            "         [ 1.2052e-01, -8.7394e-03, -1.0463e-01,  ..., -1.3054e-01,\n",
            "          -1.0206e-01, -3.6783e-02]]], grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "multi_attn = MultiheadAttention(d_model, d_k, num_heads)\n",
        "outputs = multi_attn(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}