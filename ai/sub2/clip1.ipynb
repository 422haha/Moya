{"cells":[{"cell_type":"markdown","metadata":{"id":"5gUMDNr-pCwD"},"source":["# 과제 목표\n","\n","1. 대규모 데이터로 미리 학습된 CLIP 모델로 이미지 혹은 텍스트의 CLIP feature를 추출하는 방법을 학습\n","2. CLIP 모델을 활용하여 비디오 속 프레임을 검색하는 기능을 구현"]},{"cell_type":"markdown","metadata":{"id":"oDYVkQhVT5yH"},"source":["# 환경 세팅"]},{"cell_type":"markdown","metadata":{},"source":["본 실습에서 사용할 라이브러리를 import 하겠습니다. Colab을 사용할 경우, 아래 명령어로 CLIP을 추가 설치해주세요."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Colab 사용 시\n","!pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9779,"status":"ok","timestamp":1718212918224,"user":{"displayName":"한은기","userId":"13693550981734959354"},"user_tz":-540},"id":"yXPbd-Qbxe69"},"outputs":[],"source":["import math\n","from tqdm import tqdm\n","import torch\n","import torchvision\n","import torchvision.transforms.functional as F\n","import clip\n","import cv2\n","from PIL import Image\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["Colab을 사용할 경우, 검색에 사용할 비디오를 자신의 구글 드라이브에 저장하고 이 비디오를 불러올 것입니다. 이를 위하여 구글 드라이브를 코랩 파일에 연결하겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16954,"status":"ok","timestamp":1718212969469,"user":{"displayName":"한은기","userId":"13693550981734959354"},"user_tz":-540},"id":"5qlXXTVpw5UH","outputId":"53a5f31c-0c4f-4e1d-dc44-3bd7960e63d8"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{},"source":["# CLIP으로 영상 속 프레임 검색기 만들기"]},{"cell_type":"markdown","metadata":{},"source":["CLIP 활용의 실습으로, 비디오 내에서 내가 원하는 프레임을 검색해 보여주는 시스템을 만들어보겠습니다.\n","\n","실습에 사용할 동영상은 자신이 원하는 것을 사용하면 됩니다. 손쉽게 영상을 얻을 수 있는 링크를 아래와 같이 첨부합니다.\n","\n","- [Pexels](https://www.pexels.com/ko-kr/), [Pixabay](https://pixabay.com/): '동영상' 옵션 선택 후 원하는 영상을 검색\n","- [Artlist](https://artlist.io/stock-footage)\n","- [한국저작권위원회 공유마당](https://gongu.copyright.or.kr/gongu/wrt/wrtCl/listWrtVideo.do?menuNo=200026): 다양한 장면을 시험하기 위해 '영화' 카테고리 추천."]},{"cell_type":"markdown","metadata":{"id":"61sUQSFST9Tv"},"source":["본 실습에 사용할 설정값을 지정합니다. 아래 `video_path`는 자신이 다운받은 비디오의 구글드라이브 링크로 바꾸고, `start_frame` 등 다른 설정값도 자신에 맞게 수정해주세요."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1718212918225,"user":{"displayName":"한은기","userId":"13693550981734959354"},"user_tz":-540},"id":"CkivNCm5T83d"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","video_path = 'video.mp4' # colab path example: 'drive/MyDrive/project/sub2/video2.mp4'\n","start_frame = 0 # start of video frame extraction\n","end_frame = 0 # end of video frame extraction. 0 to end (max) of video frames\n","n_skip_frame = 0 # interval to extract frames in the video. 0 for all frames\n","batch_size = 256"]},{"cell_type":"markdown","metadata":{"id":"84o7xRCGUOCp"},"source":["본 실습에서는 huggingface에서 제공하는 CLIP 모델을 사용합니다. 사용 예시와 다양한 모델에 대한 docs는 아래 링크를 참조해주세요!\n","\n","https://huggingface.co/docs/transformers/model_doc/clip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15128,"status":"ok","timestamp":1718212933347,"user":{"displayName":"한은기","userId":"13693550981734959354"},"user_tz":-540},"id":"UvIomUdPzXib","outputId":"41929da9-4f45-4dad-bf40-0f1ac9469016"},"outputs":[],"source":["# CLIP 모델 및 이미지 프로세서 불러오기\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)"]},{"cell_type":"markdown","metadata":{"id":"NC2QN0f3pdNj"},"source":["## 1. 검색에 사용할 비디오 불러오기\n","\n","CLIP은 PIL 형태의 이미지를 입력으로 받아 처리합니다. load_video 함수를 구현하여 비디오에서 각 프레임을 받아올 수 있도록 합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14941,"status":"ok","timestamp":1718212988590,"user":{"displayName":"한은기","userId":"13693550981734959354"},"user_tz":-540},"id":"tVmQBxqpCAX7","outputId":"78527968-af4d-4a67-db35-7de294841abf"},"outputs":[],"source":["############################################################################\n","# Req 1-1: 검색에 사용할 비디오 불러오기                                        #\n","############################################################################\n","\n","def load_video(video_path, start_frame, end_frame, n_skip_frame):\n","    \"\"\"\n","    Args:\n","        video_path (str): path of video file\n","        start_frame, end_frame (int): start/end frame of video frame extraction\n","        n_skip_frame (int): interval to extract frames in the video. 0 for all frames\n","    Returns:\n","        frames (list[PIL.Image.Image]): list of video frames\n","        fps (int): FPS of video\n","    \"\"\"\n","    ################################################################################\n","    # TODO: 비디오를 불러와 각 프레임을 PIL.Image 형태로 변경함                          #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    # 1) OpenCV의 VideoCapture를 이용하여 비디오를 열 수 있게 조치함\n","    # 2) start_frame부터 end_frame까지 n_skip_frame 간격으로 돌면서 각 프레임을 얻고, PIL Image로 변환함\n","\n","    frames = [] # list[PIL.Image.Image]\n","\n","    # OpenCV의 VideoCapture를 이용하여 비디오를 열 수 있게 조치함\n","    capture = \"\"\"Write your code\"\"\"\n","    fps = capture.get(cv2.CAP_PROP_FPS) # FPS\n","\n","    length = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)) # Number of frames\n","    end_frame = length if end_frame == 0 else end_frame\n","    n_skip_frame = 1 if n_skip_frame == 0 else n_skip_frame\n","\n","    # start_frame부터 end_frame까지 n_skip_frame 간격으로 돌면서 각 프레임을 얻고, PIL Image로 변환함\n","    # 한 프레임은 (H, W, 3) 형상이며, 픽셀값이 [0, 255] 범위를 가짐.\n","    \"\"\"Write your code\"\"\"\n","    \n","    return frames, fps\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ################################################################################\n","    #                                 END OF YOUR CODE                             #\n","    ################################################################################"]},{"cell_type":"markdown","metadata":{},"source":["아래 코드로 `load_video` 함수를 테스트할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["frames, fps = load_video(video_path, start_frame, end_frame, n_skip_frame)\n","num_frame = len(frames)\n","print(f\"Extracted Frames: {num_frame} ({(num_frame/fps):.2f} sec.)\")"]},{"cell_type":"markdown","metadata":{"id":"Su_eyYM4p0xt"},"source":["## 2. 이미지의 CLIP feature 추출하기\n","\n","각 프레임을 CLIP에 넣어 이미지 특징벡터를 추출하겠습니다.\n","\n","사용자가 원하는 프레임의 텍스트 정보를 입력했을 때, 이미지 CLILP feature와 텍스트 CLIP feature 간의 유사도를 검사해 가장 높은 score를 가지는 프레임이 사용자가 원하는 프레임이라 할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12262,"status":"ok","timestamp":1718213020387,"user":{"displayName":"한은기","userId":"13693550981734959354"},"user_tz":-540},"id":"M9LTj4wi2FX4","outputId":"5cee0be4-dab9-4223-a7cf-8924a3f2d88a"},"outputs":[],"source":["############################################################################\n","# Req 1-2: 이미지의 CLIP feature 추출하기                                     #\n","############################################################################\n","\n","def get_video_feat(model, preprocess, frames, batch_size, device):\n","    \"\"\"\n","    Args:\n","        model (clip.model.CLIP): CLIP model\n","        preprocess (torchvision.transforms.transforms.Compose): preprocessing procedure of CLIP\n","        frames (list[PIL.Image.Image]): List of frames to get features\n","        batch_size (int): number of images to process within one batch\n","        device (str): device to run the model\n","    Returns:\n","        video_features (torch.Tensor): CLIP image features of input frames\n","    \"\"\"\n","    ################################################################################\n","    # TODO: 각 비디오 프레임의 CLIP feature를 추출하고 normalize함                      #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    # 1) 비디오 프레임을 배치 크기로 나눴을 때, 몇 개의 배치가 나오는지 계산하기\n","    # 2) 배치 개수 만큼 아래 과정을 반복하며, feature를 저장하기\n","    #    (1) 배치 크기 만큼의 프레임을 가져오기\n","    #    (2) 이미지 전처리하기 & 텐서화 & device 지정하기\n","    #    (3) CLIP으로 이미지 피처 추출하기: encode_image() 함수 사용\n","    #    (4) 위 결과를 normalize하기\n","    \n","    \"\"\"Write your code\"\"\"\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ################################################################################\n","    #                                 END OF YOUR CODE                             #\n","    ################################################################################\n","    \n","    return video_features"]},{"cell_type":"markdown","metadata":{},"source":["아래 코드로 `get_video_feat` 함수를 테스트할 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["video_features = get_video_feat(model, preprocess, frames, batch_size, device)\n","print(f\"Shape of Features: {video_features.shape}\") # [num_frame, dim=512]"]},{"cell_type":"markdown","metadata":{"id":"ja44kMT--XBf"},"source":["## 3. 텍스트-이미지 CLIP feature 간의 유사도 계산하기"]},{"cell_type":"markdown","metadata":{"id":"9FH6IKLPr-7p"},"source":["입력한 텍스트에서 CLIP feature를 얻고, 이 feature와 비디오 각 프레임 image features 간의 유사도를 구하는 함수를 작성하겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":375,"status":"ok","timestamp":1718213042580,"user":{"displayName":"한은기","userId":"13693550981734959354"},"user_tz":-540},"id":"HtMyLyYt2_DE"},"outputs":[],"source":["############################################################################\n","# Req 1-3: 텍스트-이미지 CLIP feature 간의 유사도 계산하기                      #\n","############################################################################\n","\n","@torch.no_grad()\n","def similarity_score(search_query, video_features, device, k=5):\n","    \"\"\"\n","    Args:\n","        search_query (str): Search query for video frame\n","        video_features (torch.Tensor): CLIP features of video. shape = [batch, dim_feature]\n","    Returns:\n","        sim_scores (torch.Tensor): Similarity scores of range [0, 1]. The length is same with the number of video frames\n","        topk_idx (torch.Tensor): Top k score indices. The length is k\n","    \"\"\"\n","    ################################################################################\n","    # TODO: 사용자가 입력한 텍스트를 토크나이즈하여 CLIP 모델에 전달하여 text feature를 얻고 #\n","    # CLIP image featrues와의 유사도와 가장 높은 유사도의 K 개 인덱스를 추출함             #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    # 1) 텍스트 입력을 토큰화: CLIP 모델의 tokenize 사용\n","    # 2) Text CLIP feature 추출: encode_text 함수 사용\n","    # 3) 비디오 각 프레임과 텍스트 간 유사도 계산. torch.nn.CosineSimilarity 등을 이용할 수 있음.\n","    # 4) Softmax 함수를 사용하여 similarity score의 총합이 1이 되도록 함\n","    # 5) 가장 유사도가 높은 k개의 점수와 프레임 인덱스를 구함\n","\n","    \"\"\"Write your code\"\"\"\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ################################################################################\n","    #                                 END OF YOUR CODE                             #\n","    ################################################################################\n","\n","    return sim_scores, topk_idx"]},{"cell_type":"markdown","metadata":{},"source":["아래 코드로 `similarity_score` 함수를 테스트할 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["similarity_scores, topk_idx = similarity_score(\n","    \"a picture of a tiger\", video_features, device, k=5,\n",")\n","print(f\"Similarity scores: {max(similarity_scores):.5f} ~ {min(similarity_scores):.5f}\")\n","print(f\"Top-K indices: {topk_idx}\")"]},{"cell_type":"markdown","metadata":{"id":"fUZrHa1E-rCp"},"source":["## 4. 검색 결과 시각화"]},{"cell_type":"markdown","metadata":{"id":"_AX9QBtDtHuR"},"source":["아래 함수는 텍스트 입력에 각 프레임이 얼마나 해당하는지(similarity score)를 입력으로 받아 그 정도를 시각화하는 함수입니다. Heatmap의 선이 붉을수록, 그래프의 y값이 높을수록 유사도가 높습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":466,"status":"ok","timestamp":1718213044747,"user":{"displayName":"한은기","userId":"13693550981734959354"},"user_tz":-540},"id":"enlF1M9Q6ZWr"},"outputs":[],"source":["def similarity_vis(similarities):\n","\n","    similarities = similarities.cpu()\n","    \n","    fig, (ax1, ax2) = plt.subplots(nrows=2)\n","    ax1.imshow(\n","        similarities.unsqueeze(dim=0), \n","        cmap=\"magma\", \n","        aspect=\"auto\",\n","        extent=[0, len(similarities), 0, 1],\n","    )\n","    ax1.axes.yaxis.set_visible(False)\n","    ax2.plot(similarities)\n","\n","    plt.show()\n","    plt.savefig(\"similarity_vis.jpg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"executionInfo":{"elapsed":1440,"status":"ok","timestamp":1718213099051,"user":{"displayName":"한은기","userId":"13693550981734959354"},"user_tz":-540},"id":"gOBewNNw7mmy","outputId":"d50e32b7-0fde-4ce6-c9e5-7618d60d6401"},"outputs":[],"source":["similarity_vis(similarity_scores)"]},{"cell_type":"markdown","metadata":{"id":"ZmGYbyDatZdj"},"source":["top-K 개의 가장 높은 유사도를 보이는 프레임을 조회할 수 있는 `visualize_images` 함수를 구현합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1718213046096,"user":{"displayName":"한은기","userId":"13693550981734959354"},"user_tz":-540},"id":"1AnVIOmpEPaB"},"outputs":[],"source":["############################################################################\n","# Req 1-4: 검색 결과 시각화                                                  #\n","############################################################################\n","\n","def visualize_images(imgs):\n","    \"\"\"\n","    Args:\n","        img (list[PIL.image]): List of images\n","    \"\"\"\n","    ################################################################################\n","    # TODO: 이미지를 텐서화하고, 그리드를 이용하여 k 개의 이미지를 한 번에 나타냄            #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    # 1) 이미지를 PyTorch Tensor로 변경\n","    # 2) torchvision.utils.make_grid를 사용하여 그리드 안에 여러 이미지를 한 장에 담을 수 있게 함. \n","    #    한 행에 몇 개의 이미지를 담을지, 각 이미지 간의 간격을 얼마로 줄 지는 자유롭게 선택함.\n","    # 3) [C, H, W] 형태를 [H, W, C] 형태로 변경함.\n","\n","    \"\"\"Write your code\"\"\"\n","    \n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ################################################################################\n","    #                                 END OF YOUR CODE                             #\n","    ################################################################################\n","    \n","    plt.figure(figsize=(15,8))\n","    plt.imshow(imgs)\n","    plt.axis('off')\n","    plt.show()\n","    plt.savefig(\"search_result_imgs.jpg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":481},"executionInfo":{"elapsed":2719,"status":"ok","timestamp":1718213107324,"user":{"displayName":"한은기","userId":"13693550981734959354"},"user_tz":-540},"id":"OckukBs7Fhie","outputId":"d1fb8d66-e0c5-4eb5-a91b-f0e5e621d194"},"outputs":[],"source":["topk_frames = [frames[idx] for idx in topk_idx]\n","visualize_images(topk_frames)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3.12.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"vscode":{"interpreter":{"hash":"791643beab681c1899ac39bc49692ca6f416e697f983435e6ed06c1ef24d4aaf"}}},"nbformat":4,"nbformat_minor":0}
