{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gUMDNr-pCwD"
      },
      "source": [
        "# 과제 목표\n",
        "\n",
        "1. 대규모 데이터로 미리 학습된 CLIP 모델로 이미지 혹은 텍스트의 CLIP feature를 추출하는 방법을 학습\n",
        "2. CLIP 모델을 활용하여 추가 학습이 없는 제로샷 이미지 분류기 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDYVkQhVT5yH"
      },
      "source": [
        "# 환경 세팅"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ_Mzz5fUFHF"
      },
      "source": [
        "본 실습에서 사용할 라이브러리를 import 하겠습니다. Colab을 사용할 경우, 아래 명령어로 CLIP을 추가 설치해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCAxDBMnUFHG",
        "outputId": "ea61e4eb-152e-4375-db72-29bc2e026756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-q7ipr00e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-q7ipr00e\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.19.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Downloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=f2eaea9929569ffe0fc9696354300c6dba4bc2c9135cdaec2a2f713f69de13f3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dba88k3h/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.2.3\n"
          ]
        }
      ],
      "source": [
        "# Colab 사용 시\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yXPbd-Qbxe69"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import clip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvP6TvWlUFHH"
      },
      "source": [
        "# CLIP으로 제로샷 이미지 분류기 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9ao-39tUFHH"
      },
      "source": [
        "CLIP 활용의 실습으로, 기학습된 CLIP 모델로 이미지 분류기를 따로 학습하지 않고도 이미지 분류를 수행해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGU-M7byUFHH"
      },
      "source": [
        "본 실습에 사용할 설정값을 지정하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EM6vcyLAUFHH"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "cifar100_dataset_path = './datasets'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQDGfBglUFHH"
      },
      "source": [
        "본 실습에서는 huggingface에서 제공하는 CLIP 모델을 사용합니다. 사용 예시와 다양한 모델에 대한 docs는 아래 링크를 참조해주세요!\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kht6A4dUUFHH",
        "outputId": "435ecdf7-946b-4d7b-da9d-8dbd116f4eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:04<00:00, 86.7MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# CLIP 모델 및 이미지 프로세서 불러오기\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVcSRuNwUFHI"
      },
      "source": [
        "데이터셋은 SUB PJT 1에서 사용한 CIFAR-10보다 더 클래스가 많은 CIFAR-100 데이터셋을 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQgg9j7_UFHI",
        "outputId": "9bed3535-8989-407a-bc46-6cedb5ccbd3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./datasets/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:02<00:00, 74715938.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/cifar-100-python.tar.gz to ./datasets\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "dataset = torchvision.datasets.CIFAR100(\n",
        "    cifar100_dataset_path, train=False, download=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdDbUtXvUFHI"
      },
      "source": [
        "## 1. 라벨 텍스트 토큰화하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQa1qiwtY2Rl"
      },
      "source": [
        "텍스트(라벨)의 CLIP feature를 얻기 위해선 텍스트를 토큰화해야 합니다. \"a photo of a {LABEL}\" 형식으로 CIFAR-100 데이터셋의 라벨 정보를 토큰화하여 리스트로 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOBLXVWuMUi3",
        "outputId": "1353199e-e7fe-47c2-adc2-baabb091c56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels: 100\n",
            "Result of tokenization (for random 3 labels)\n",
            "tensor([49406,   320,  1125,   539,   320, 23132, 49407,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0], device='cuda:0',\n",
            "       dtype=torch.int32)\n",
            "tensor([49406,   320,  1125,   539,   320,  5567, 49407,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0], device='cuda:0',\n",
            "       dtype=torch.int32)\n",
            "tensor([49406,   320,  1125,   539,   320, 22874, 49407,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0], device='cuda:0',\n",
            "       dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "############################################################################\n",
        "# Req 2-1: 라벨 텍스트 토큰화하기                                              #\n",
        "############################################################################\n",
        "\n",
        "################################################################################\n",
        "# TODO: 데이터셋 내 라벨을 이용하여 이미지를 설명하는 텍스트를 만들고, 각 텍스트의        #\n",
        "# 토큰화를 진행함                                                                 #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# 1) 데이터셋 내 클래스(라벨)을 리스트화함\n",
        "labels = dataset.classes\n",
        "\n",
        "# 2) 클래스(라벨)을 이용하여 \"a photo of a {LABEL}\" 형태의 텍스트를 tokenize함\n",
        "text_inputs = [clip.tokenize(f\"a photo of a {label}\") for label in labels]\n",
        "text_inputs = torch.cat(text_inputs).to(device)\n",
        "\n",
        "print(f\"Number of labels: {len(text_inputs)}\")\n",
        "print(\"Result of tokenization (for random 3 labels)\")\n",
        "random_text = random.sample(list(text_inputs), 3)\n",
        "for i in range(len(random_text)):\n",
        "    print(random_text[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y97yqVPUFHI"
      },
      "source": [
        "## 2. 이미지와 라벨 텍스트 간 CLIP feature 유사도 계산하기\n",
        "\n",
        "이미지 클래스(라벨)의 CLIP text feature와 분류 대상의 입력 이미지의 CLIP image features 간 top-K 유사도를 구하는 함수를 작성하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_zfEbQHjPUSu"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "# Req 2-2: 이미지와 라벨 텍스트 간 유사도 계산하기                               #\n",
        "############################################################################\n",
        "\n",
        "def calc_similarities(image, text_inputs, topk):\n",
        "    \"\"\"Calulate cosine similarities between image features and text features.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): Image to classify the label\n",
        "        text_inputs (torch.Tensor): Tokenized texts containing label information\n",
        "        topk (int): How many indices (classes) to extract as an answer\n",
        "    Returns:\n",
        "        top_sim_scores (torch.Tensor): Top-K similarity scores\n",
        "        top_indices (torch.Tensor): Top-K indices (order of nubmers matched with `top_sim_scores`)\n",
        "    \"\"\"\n",
        "    ################################################################################\n",
        "    # TODO: 이미지를 텐서화하고, 그리드를 이용하여 k 개의 이미지를 한 번에 나타냄            #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # 1) 이미지를 CLIP 전처리를 통하여 전처리함\n",
        "    processed_image = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # 2) 라벨 정보를 담은 텍스트와 입력 이미지 각각의 CLIP feature를 구함\n",
        "    image_features = model.encode_image(processed_image)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "    # 3) Image feature와 text feature의 코사인 유사도를 구함\n",
        "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "    similarities = torch.matmul(image_features, text_features.T).squeeze(0)\n",
        "\n",
        "    # 4) 코사인 유사도에 softmax를 취하여 총합을 1로 만듦\n",
        "    sim_scores = torch.nn.functional.softmax(similarities, dim=-1)\n",
        "\n",
        "    # 5) Top-K 값과 이에 해당하는 인덱스를 추출함\n",
        "    top_sim_scores, top_indices = sim_scores.topk(topk, dim=-1)\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        "\n",
        "    return top_sim_scores, top_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcwr1-G4Zb_R"
      },
      "source": [
        "아래 코드로 `calc_similarities` 함수를 테스트할 수 있습니다.\n",
        "\n",
        "이미지 분류를 테스트할 대상 이미지를 무작위로 선택하고, 몇 개의 분류 예측을 뽑을 것인지 Top-K 숫자도 지정합니다. 이후 `calc_similarities` 함수를 이용하여 예측 결과를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "kk6CSljZTdk-",
        "outputId": "9fb64150-4d19-4e70-e373-c4688d9c7beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label (answer): elephant\n",
            "\n",
            "Top 10 Classification Results:\n",
            "elephant       : 1.04%\n",
            "chimpanzee     : 1.04%\n",
            "camel          : 1.04%\n",
            "cattle         : 1.03%\n",
            "kangaroo       : 1.03%\n",
            "willow_tree    : 1.02%\n",
            "flatfish       : 1.02%\n",
            "palm_tree      : 1.02%\n",
            "lawn_mower     : 1.02%\n",
            "bear           : 1.02%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Select random image from dataset\n",
        "image_idx = random.randint(0, len(dataset))\n",
        "image, label_idx = dataset[image_idx]\n",
        "\n",
        "# Draw image\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(image)\n",
        "\n",
        "# Select K to show Top-K classification result\n",
        "K = 10\n",
        "\n",
        "# Calculate the similarites between image and labels\n",
        "# and classify the image with similarity scores\n",
        "sim_scores, indices = calc_similarities(image, text_inputs, K)\n",
        "\n",
        "print(f\"Label (answer): {dataset.classes[label_idx]}\\n\")\n",
        "print(f\"Top {K} Classification Results:\")\n",
        "for score, idx in zip(sim_scores, indices):\n",
        "    print(f\"{dataset.classes[idx]:15s}: {score.item()*100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}