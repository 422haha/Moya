{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gUMDNr-pCwD"
      },
      "source": [
        "# 과제 목표\n",
        "\n",
        "1. 대규모 데이터로 미리 학습된 CLIP 모델로 이미지 혹은 텍스트의 CLIP feature를 추출하는 방법을 학습\n",
        "2. CLIP 모델을 활용하여 추가 학습이 없는 제로샷 이미지 분류기 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDYVkQhVT5yH"
      },
      "source": [
        "# 환경 세팅"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phPa8_KN65kq"
      },
      "source": [
        "본 실습에서 사용할 라이브러리를 import 하겠습니다. Colab을 사용할 경우, 아래 명령어로 CLIP을 추가 설치해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSfr39xZ65kq",
        "outputId": "e71d2835-a297-4a9d-fb85-febc7c4f6493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-v2yu_gg5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-v2yu_gg5\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.19.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Downloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=21447eb8688d857c872e0786a2a047b6a6267b3e02ea39d528e06df7dd46a11e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xsq_6mqn/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.2.3\n"
          ]
        }
      ],
      "source": [
        "# Colab 사용 시\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yXPbd-Qbxe69"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import clip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRSdacW365kq"
      },
      "source": [
        "# CLIP으로 제로샷 이미지 분류기 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfmV-7T_65kq"
      },
      "source": [
        "CLIP 활용의 실습으로, 기학습된 CLIP 모델로 이미지 분류기를 따로 학습하지 않고도 이미지 분류를 수행해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2mmpGpc65kr"
      },
      "source": [
        "본 실습에 사용할 설정값을 지정하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NGTyN8mt65kr"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "cifar100_dataset_path = './datasets'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myq_NCjI65kr"
      },
      "source": [
        "본 실습에서는 huggingface에서 제공하는 CLIP 모델을 사용합니다. 사용 예시와 다양한 모델에 대한 docs는 아래 링크를 참조해주세요!\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp4VeGB665kr",
        "outputId": "750f94e7-8417-41ee-9c78-8906606644d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 96.1MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# CLIP 모델 및 이미지 프로세서 불러오기\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spO5gE1a65kr"
      },
      "source": [
        "데이터셋은 SUB PJT 1에서 사용한 CIFAR-10보다 더 클래스가 많은 CIFAR-100 데이터셋을 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fajL5GF65kr",
        "outputId": "a0d2c7ed-c45b-40ed-de1f-127c0e771322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./datasets/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:05<00:00, 30766949.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/cifar-100-python.tar.gz to ./datasets\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "dataset = torchvision.datasets.CIFAR100(\n",
        "    cifar100_dataset_path, train=False, download=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OVpG8jJ65kr"
      },
      "source": [
        "## 1. 라벨 텍스트 토큰화하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQa1qiwtY2Rl"
      },
      "source": [
        "텍스트(라벨)의 CLIP feature를 얻기 위해선 텍스트를 토큰화해야 합니다. \"a photo of a {LABEL}\" 형식으로 CIFAR-100 데이터셋의 라벨 정보를 토큰화하여 리스트로 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOBLXVWuMUi3",
        "outputId": "cfc15673-d5cb-411f-b210-0774fac3189a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels: 100\n",
            "Result of tokenization (for random 3 labels)\n",
            "tensor([49406,   320,  1125,   539,   320,   786, 49407,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0], device='cuda:0',\n",
            "       dtype=torch.int32)\n",
            "tensor([49406,   320,  1125,   539,   320, 10945, 49407,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0], device='cuda:0',\n",
            "       dtype=torch.int32)\n",
            "tensor([49406,   320,  1125,   539,   320,  8798, 49407,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0], device='cuda:0',\n",
            "       dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "############################################################################\n",
        "# Req 2-1: 라벨 텍스트 토큰화하기                                              #\n",
        "############################################################################\n",
        "\n",
        "################################################################################\n",
        "# TODO: 데이터셋 내 라벨을 이용하여 이미지를 설명하는 텍스트를 만들고, 각 텍스트의        #\n",
        "# 토큰화를 진행함                                                                 #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# 1) 데이터셋 내 클래스(라벨)을 리스트화함\n",
        "# 2) 클래스(라벨)을 이용하여 \"a photo of a {LABEL}\" 형태의 텍스트를 tokenize함\n",
        "\n",
        "\n",
        "# 데이터셋 내 클래스(라벨)을 리스트화함\n",
        "labels = dataset.classes\n",
        "\n",
        "# 클래스(라벨)을 이용하여 \"a photo of a {LABEL}\" 형태의 텍스트를 tokenize함\n",
        "text_inputs = [clip.tokenize(f\"a photo of a {label}\") for label in labels]\n",
        "text_inputs = torch.cat(text_inputs).to(device)\n",
        "\n",
        "print(f\"Number of labels: {len(text_inputs)}\")\n",
        "print(\"Result of tokenization (for random 3 labels)\")\n",
        "random_text = random.sample(list(text_inputs), 3)\n",
        "for i in range(len(random_text)):\n",
        "    print(random_text[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iNGtXHD65ks"
      },
      "source": [
        "## 2. 이미지와 라벨 텍스트 간 CLIP feature 유사도 계산하기\n",
        "\n",
        "이미지 클래스(라벨)의 CLIP text feature와 분류 대상의 입력 이미지의 CLIP image features 간 top-K 유사도를 구하는 함수를 작성하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_zfEbQHjPUSu"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "# Req 2-2: 이미지와 라벨 텍스트 간 유사도 계산하기                               #\n",
        "############################################################################\n",
        "\n",
        "def calc_similarities(image, text_inputs, topk):\n",
        "    \"\"\"Calulate cosine similarities between image features and text features.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): Image to classify the label\n",
        "        text_inputs (torch.Tensor): Tokenized texts containing label information\n",
        "        topk (int): How many indices (classes) to extract as an answer\n",
        "    Returns:\n",
        "        top_sim_scores (torch.Tensor): Top-K similarity scores\n",
        "        top_indices (torch.Tensor): Top-K indices (order of nubmers matched with `top_sim_scores`)\n",
        "    \"\"\"\n",
        "    ################################################################################\n",
        "    # TODO: 이미지를 텐서화하고, 그리드를 이용하여 k 개의 이미지를 한 번에 나타냄            #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # 1) 이미지를 CLIP 전처리를 통하여 전처리함\n",
        "    image_preprocessed = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # 2) 라벨 정보를 담은 텍스트와 입력 이미지 각각의 CLIP feature를 구함\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image_preprocessed)\n",
        "        text_features = model.encode_text(text_inputs)\n",
        "\n",
        "    # 3) Image feature와 text feature의 코사인 유사도를 구함\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarities = (image_features @ text_features.T).squeeze(0)\n",
        "\n",
        "    # 4) 코사인 유사도에 softmax를 취하여 총합을 1로 만듦\n",
        "    softmax_similarities = torch.softmax(similarities, dim=-1)\n",
        "\n",
        "    # 5) Top-K 값과 이에 해당하는 인덱스를 추출함\n",
        "    top_sim_scores, top_indices = torch.topk(softmax_similarities, topk)\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        "\n",
        "    return top_sim_scores, top_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcwr1-G4Zb_R"
      },
      "source": [
        "아래 코드로 `calc_similarities` 함수를 테스트할 수 있습니다.\n",
        "\n",
        "이미지 분류를 테스트할 대상 이미지를 무작위로 선택하고, 몇 개의 분류 예측을 뽑을 것인지 Top-K 숫자도 지정합니다. 이후 `calc_similarities` 함수를 이용하여 예측 결과를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "kk6CSljZTdk-",
        "outputId": "01908b5c-5a4e-4a63-b3f9-66d55ae52423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label (answer): palm_tree\n",
            "\n",
            "Top 10 Classification Results:\n",
            "palm_tree      : 1.09%\n",
            "pine_tree      : 1.06%\n",
            "willow_tree    : 1.05%\n",
            "oak_tree       : 1.04%\n",
            "maple_tree     : 1.04%\n",
            "house          : 1.03%\n",
            "lawn_mower     : 1.03%\n",
            "aquarium_fish  : 1.02%\n",
            "lamp           : 1.02%\n",
            "sweet_pepper   : 1.02%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Select random image from dataset\n",
        "image_idx = random.randint(0, len(dataset))\n",
        "image, label_idx = dataset[image_idx]\n",
        "\n",
        "# Draw image\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(image)\n",
        "\n",
        "# Select K to show Top-K classification result\n",
        "K = 10\n",
        "\n",
        "# Calculate the similarites between image and labels\n",
        "# and classify the image with similarity scores\n",
        "sim_scores, indices = calc_similarities(image, text_inputs, K)\n",
        "\n",
        "print(f\"Label (answer): {dataset.classes[label_idx]}\\n\")\n",
        "print(f\"Top {K} Classification Results:\")\n",
        "for score, idx in zip(sim_scores, indices):\n",
        "    print(f\"{dataset.classes[idx]:15s}: {score.item()*100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}